## Bigram Language Model Implementation

### Student Information

Name: Abhinay Rao Gandra 

Course: CS5760 â€“ Natural Language Processing  

Semester: Spring 2026  

---

## ğŸ“Œ Project Description

This project implements a Bigram Language Model using Maximum Likelihood Estimation (MLE).  
The model is trained on a small corpus:

```bash
<s> I love NLP </s>  
<s> I love deep learning </s>  
<s> deep learning is fun </s>  
```

The program:

- Computes unigram counts
- Computes bigram counts
- Estimates bigram probabilities using MLE
- Calculates sentence probabilities
- Determines which sentence is more probable

---

## ğŸ§  Mathematical Formulation

Bigram probability is computed using:

P(wâ‚‚ | wâ‚) = Count(wâ‚, wâ‚‚) / Count(wâ‚)

Sentence probability is:

P(sentence) = âˆ P(wáµ¢ | wáµ¢â‚‹â‚)

---

## ğŸ“‚ Files Included

- `Q5.Bigram_Language_Model.py` â†’ Main implementation
- `README.md` â†’ Project documentation

---

## â–¶ï¸ How to Run

1. Clone the repository

2. Navigate to the project directory


3. Run the program:
```bash
python3 Q5.Bigram_Language_Model.py
```


---

## ğŸ“Š Expected Output

The program prints:

- Probability of sentence 1
- Probability of sentence 2
- Which sentence the model prefers
- Explanation based on higher probability

---

## ğŸ“– Key Concepts Used

- N-gram Language Models
- Maximum Likelihood Estimation (MLE)
- Unigram & Bigram counts
- Sentence probability computation

---

## âœ… Conclusion

The model prefers the sentence with the higher product of bigram probabilities.  
This demonstrates how statistical language models rank sentences based on learned corpus statistics.

---
